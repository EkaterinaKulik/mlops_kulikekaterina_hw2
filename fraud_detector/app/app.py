# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18RIRYukk9NXNbK6HmJd_am2tkNriBqGR
"""

import os
import sys
import json
import logging
import signal
from typing import Any, Dict, Optional

from confluent_kafka import Consumer, Producer
import pandas as pd

sys.path.append(os.path.abspath("./src"))
from preprocessing import preprocess_one
from scorer import score_one, is_fraud


KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")
IN_TOPIC = os.getenv("KAFKA_TRANSACTIONS_TOPIC", "transactions")
OUT_TOPIC = os.getenv("KAFKA_SCORES_TOPIC", "scores")
GROUP_ID = os.getenv("KAFKA_GROUP_ID", "ml-scorer")
AUTO_OFFSET_RESET = os.getenv("KAFKA_AUTO_OFFSET_RESET", "earliest")  # earliest|latest
POLL_TIMEOUT_SEC = float(os.getenv("KAFKA_POLL_TIMEOUT_SEC", "1.0"))
FRAUD_THRESHOLD = float(os.getenv("FRAUD_THRESHOLD", "0.5"))

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("fraud_stream_service")

def _parse_message_value(raw: bytes) -> Dict[str, Any]:

    obj = json.loads(raw.decode("utf-8"))

    if "data" in obj and isinstance(obj["data"], dict):
        rec = obj["data"].copy()
        if "transaction_id" in obj and "transaction_id" not in rec:
            rec["transaction_id"] = obj["transaction_id"]
        return rec

    return obj


def _build_output_message(transaction_id: Any, score: float, threshold: float) -> Dict[str, Any]:
    return {
        "transaction_id": transaction_id,
        "score": score,
        "fraud_flag": int(score >= threshold)
    }


class ProcessingService:
    def __init__(self) -> None:
        self.consumer = Consumer({
            "bootstrap.servers": KAFKA_BOOTSTRAP_SERVERS,
            "group.id": GROUP_ID,
            "auto.offset.reset": AUTO_OFFSET_RESET,
            "enable.auto.commit": False,
        })
        self.producer = Producer({"bootstrap.servers": KAFKA_BOOTSTRAP_SERVERS})

        self.consumer.subscribe([IN_TOPIC])
        logger.info(f"Subscribed to topic: {IN_TOPIC}")

        self._running = True

    def _delivery_report(self, err, msg) -> None:
        if err is not None:
            logger.error(f"Produce failed: {err}")
        else:
            logger.debug(
                f"Produced to {msg.topic()} [partition {msg.partition()}] at offset {msg.offset()}"
            )

    def _graceful_stop(self, *_):
        logger.info("Stop signal received. Shutting down...")
        self._running = False

    def process_messages(self) -> None:
        signal.signal(signal.SIGINT, self._graceful_stop)
        signal.signal(signal.SIGTERM, self._graceful_stop)

        while self._running:
            msg = self.consumer.poll(POLL_TIMEOUT_SEC)
            if msg is None:
                continue
            if msg.error():
                logger.error(f"Kafka error: {msg.error()}")
                continue

            try:
                record = _parse_message_value(msg.value())

                transaction_id: Optional[Any] = record.get("transaction_id", None)
                if transaction_id is None:
                    logger.warning("Incoming message without transaction_id. Will proceed, but add null id.")

                X: pd.DataFrame = preprocess_one(record)

                score = score_one(X)
                flag = is_fraud(score, FRAUD_THRESHOLD)

                out_msg = {
                    "transaction_id": transaction_id,
                    "score": score,
                    "fraud_flag": flag
                }
                payload = json.dumps(out_msg, ensure_ascii=False)
                self.producer.produce(
                    OUT_TOPIC,
                    key=str(transaction_id) if transaction_id is not None else None,
                    value=payload.encode("utf-8"),
                    callback=self._delivery_report
                )
                self.producer.flush()
                self.consumer.commit(asynchronous=False)

                logger.info(f"Scored transaction_id={transaction_id} score={score:.6f} flag={flag}")

            except Exception as e:
                logger.exception(f"Error processing message: {e}")

        try:
            self.consumer.close()
        except Exception:
            pass
        try:
            self.producer.flush()
        except Exception:
            pass
        logger.info("Service stopped.")

if __name__ == "__main__":
    logger.info(
        f"Starting Kafka ML scoring service. "
        f"brokers={KAFKA_BOOTSTRAP_SERVERS}, in={IN_TOPIC}, out={OUT_TOPIC}, group={GROUP_ID}"
    )
    svc = ProcessingService()
    svc.process_messages()